import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
#from models.student_model_shared_encoder import StudentNet
from models.student_model_4 import StudentNet
from models.transunet_v2_localvit_final_fixed_v6_fixed import TransUNetV2
from models.attention_unet import AttU_Net
from utils.metrics_gray import dice_score, iou_score, hd95
from utils.teacher_selector import select_dominant_teacher
from utils.uncertainty_utils import compute_uncertainty
from utils.ema_utils import smart_ema_update
from tqdm import tqdm
from utils.select_dominant_teacher import select_dominant_teacher
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from skimage.morphology import binary_erosion
from utils.calculate import ai
# ==== Config ====
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
IMG_SIZE = (224, 224)
EPOCHS = 50
WARMUP_EPOCHS = 10
BATCH_SIZE = 4
LR = 1e-4
CHECKPOINT_DIR = "/data16t/sihan/DDT/outputs/checkpoints"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# ==== Dataset ====
# class GraySegDataset(Dataset):
#     def __init__(self, img_dir, label_dir, transform=None):
#         self.image_paths = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir)])
#         self.label_paths = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir)])
#         self.transform = transform
#
#     def __len__(self):
#         return len(self.image_paths)
#
#     def __getitem__(self, idx):
#         img = Image.open(self.image_paths[idx]).convert("L").resize(IMG_SIZE)
#         mask = Image.open(self.label_paths[idx]).convert("L").resize(IMG_SIZE)
#         if self.transform:
#             img = self.transform(img)
#             mask = self.transform(mask)
#         return img, mask

class GraySegDataset(Dataset):
    def __init__(self, img_dir, label_dir=None, transform=None):
        self.image_paths = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir)])
        self.label_paths = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir)]) if label_dir else None
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img = Image.open(self.image_paths[idx]).convert("L").resize(IMG_SIZE)
        img = self.transform(img) if self.transform else img

        if self.label_paths:
            mask = Image.open(self.label_paths[idx]).convert("L").resize(IMG_SIZE)
            mask = self.transform(mask) if self.transform else mask
        else:
            mask = torch.zeros_like(img)  # è¿”å›å‡æ ‡ç­¾ä»¥å…¼å®¹ zip ç»“æ„

        return img, mask

#transform = transforms.Compose([transforms.ToTensor()])
transform = transforms.Compose([
    transforms.Resize(IMG_SIZE),
    transforms.ToTensor(),
])

labeled_dataset = GraySegDataset("/data16t/sihan/DDT/data/image/train/labeled", "/data16t/sihan/DDT/data/image/train/labels", transform=transform)
unlabeled_dataset = GraySegDataset("/data16t/sihan/DDT/data/image/train/unlabeled", None, transform=transform)
val_dataset = GraySegDataset("/data16t/sihan/DDT/data/image/val/images", "/data16t/sihan/DDT/data/image/val/labels", transform=transform)

labeled_loader = DataLoader(labeled_dataset, batch_size=BATCH_SIZE, shuffle=True)
unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)


# def safe_load_transunet(model, ckpt_path):
#     print(f"ğŸ§  æ­£åœ¨å®‰å…¨åŠ è½½æ•™å¸ˆæ¨¡å‹æƒé‡ï¼š{ckpt_path}")
#     state_dict = torch.load(ckpt_path, map_location='cpu')
#
#     # è¿‡æ»¤æ‰ä¸conv1ç›¸å…³çš„æ‰€æœ‰æƒé‡
#     filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith("conv1")}
#
#     # åŠ è½½å…¶ä½™æƒé‡
#     missing, unexpected = model.load_state_dict(filtered_state_dict, strict=False)
#
#     print("âœ… conv1 æƒé‡å·²å¿½ç•¥ï¼Œå…¶ä»–æƒé‡åŠ è½½å®Œæ¯•")
#     print("ğŸŸ¡ Missing keys:", missing)
#     print("ğŸ”µ Unexpected keys:", unexpected)

def safe_load_transunet(model, ckpt_path):
    print(f"ğŸ§  æ­£åœ¨å®‰å…¨åŠ è½½æ•™å¸ˆæ¨¡å‹æƒé‡ï¼š{ckpt_path}")
    state_dict = torch.load(ckpt_path, map_location='cpu')

    # â—ä¸¥æ ¼æ’é™¤æ‰€æœ‰ conv1 å­å±‚ï¼Œå¦‚ conv1.0.weightã€conv1.1.running_mean ç­‰
    filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith("conv1.")}

    missing, unexpected = model.load_state_dict(filtered_state_dict, strict=False)

    print("âœ… conv1 æƒé‡å·²å½»åº•å¿½ç•¥")
    print("ğŸŸ¡ Missing keys:", missing)
    print("ğŸ”µ Unexpected keys:", unexpected)



def compute_boundary_loss(pred, target, threshold=0.5):
    """
    è®¡ç®—è¾¹ç•ŒæŸå¤±ï¼Œä½¿ç”¨è…èš€æ“ä½œæ¥æå–è¾¹ç•Œ
    pred: æ¨¡å‹çš„é¢„æµ‹ç»“æœ
    target: çœŸå®æ ‡ç­¾
    threshold: ç”¨äºå°†é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾è½¬åŒ–ä¸ºäºŒå€¼å›¾åƒçš„é˜ˆå€¼
    """
    # å°†é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾äºŒå€¼åŒ–
    pred = (pred > threshold).float()
    target = (target > threshold).float()

    # å¦‚æœè¾“å…¥æ˜¯ 3D å¼ é‡ï¼Œæ·»åŠ ä¸€ä¸ª batch ç»´åº¦ï¼Œä½¿å…¶æˆä¸º 4D å¼ é‡
    if len(pred.shape) == 3:
        pred = pred.unsqueeze(0)  # è½¬æ¢ä¸º [1, C, H, W]
    if len(target.shape) == 3:
        target = target.unsqueeze(0)  # è½¬æ¢ä¸º [1, C, H, W]

    # ä½¿ç”¨å·ç§¯æ¨¡æ‹Ÿè…èš€æ“ä½œ
    kernel = torch.ones(1, 1, 3, 3).to(pred.device)  # ä½¿ç”¨ 3x3 å·ç§¯æ ¸
    pred_boundary = F.conv2d(pred, kernel, padding=1)  # å·ç§¯æ“ä½œæ¥æ¨¡æ‹Ÿè…èš€
    target_boundary = F.conv2d(target, kernel, padding=1)

    # è®¡ç®—è¾¹ç•ŒæŸå¤±
    pred_boundary = (pred_boundary > 0).float()  # å¯¹å·ç§¯ç»“æœè¿›è¡Œé˜ˆå€¼å¤„ç†
    target_boundary = (target_boundary > 0).float()

    boundary_loss = F.mse_loss(pred_boundary, target_boundary)  # ä½¿ç”¨MSEæŸå¤±è®¡ç®—è¾¹ç•ŒæŸå¤±

    return boundary_loss

# è®¡ç®— Hausdorff æŸå¤±
def hausdorff_distance(pred, target, threshold=0.5):
    """
    è®¡ç®— Hausdorff è·ç¦»æŸå¤±ï¼Œç¡®ä¿è¾¹ç•Œæ›´ç²¾ç¡®
    pred: æ¨¡å‹çš„é¢„æµ‹ç»“æœ
    target: çœŸå®æ ‡ç­¾
    threshold: ç”¨äºäºŒå€¼åŒ–å›¾åƒçš„é˜ˆå€¼
    """
    pred = (pred > threshold).float()
    target = (target > threshold).float()

    # ä½¿ç”¨å·ç§¯æ¨¡æ‹Ÿè…èš€æ“ä½œæ¥æå–è¾¹ç•Œ
    kernel = torch.ones(1, 1, 3, 3).to(pred.device)  # ä½¿ç”¨ 3x3 å·ç§¯æ ¸
    pred_boundary = F.conv2d(pred, kernel, padding=1)  # å·ç§¯æ“ä½œæ¥æ¨¡æ‹Ÿè…èš€
    target_boundary = F.conv2d(target, kernel, padding=1)

    # è®¡ç®—è¾¹ç•ŒæŸå¤±ï¼ˆä½¿ç”¨MSEï¼‰
    pred_boundary = (pred_boundary > 0).float()  # å¯¹å·ç§¯ç»“æœè¿›è¡Œé˜ˆå€¼å¤„ç†
    target_boundary = (target_boundary > 0).float()

    boundary_loss = F.mse_loss(pred_boundary, target_boundary)  # ä½¿ç”¨MSEæŸå¤±è®¡ç®—è¾¹ç•ŒæŸå¤±

    return boundary_loss


# ==== Models ====
#student = StudentNet().to(DEVICE)
student = StudentNet(
    img_size=224,
    num_classes=1,
    vit_weights="/data16t/sihan/DDT/weights/vit_base_patch16_224.pth"
).to(DEVICE)

#teacher_A = TransUNetV2(img_size=224, in_channels=1, num_classes=1).to(DEVICE)
# teacher_A = TransUNetV2(
#     img_size=224,
#     in_channels=1,
#     num_classes=1,
#     vit_weights="/data16t/sihan/DDT/weights/vit_base_patch16_224.pth",
#     pretrained_ckpt="/data16t/sihan/DDT/outputs/checkpoints/transunetv2_pretrained_full2500_with_val.pth"
# ).to(DEVICE)
teacher_A = TransUNetV2(
    img_size=224,
    in_channels=1,
    num_classes=1,
    vit_weights="/data16t/sihan/DDT/weights/vit_base_patch16_224.pth"  # ä»…åŠ è½½ViTæƒé‡
).to(DEVICE)
safe_load_transunet(teacher_A, "/data16t/sihan/DDT/outputs/checkpoints/transunetv2_pretrained_full2500_with_val.pth")
# ä¸´æ—¶åŠ å…¥ï¼šæ‰“å° conv1 å®é™…ç»“æ„

print("teacher_A.conv1[0]:", teacher_A.conv1[0])
print("ğŸ”¥ After loading weights - conv1 weight shape:", teacher_A.conv1[0].weight.shape)


teacher_B = AttU_Net(img_ch=1, output_ch=1).to(DEVICE)

# Load teacher pretrained weights
#teacher_A.load_state_dict(torch.load("/data16t/sihan/DDT/outputs/checkpoints/transunet_best.pth"))
teacher_B.load_state_dict(torch.load("/data16t/sihan/DDT/outputs/checkpoints/attunet_best.pth"))
teacher_A.eval()
teacher_B.eval()

# Optimizer
optimizer = torch.optim.Adam(student.parameters(), lr=LR)
scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.1, verbose=True)
#bce = nn.BCELoss()
bce = nn.BCEWithLogitsLoss()


best_dice = 0

# ==== Training Loop ====
for epoch in range(EPOCHS):
    student.train()
    loop = zip(labeled_loader, unlabeled_loader)
    pbar = tqdm(loop, total=min(len(labeled_loader), len(unlabeled_loader)), desc=f"[Epoch {epoch+1}/{EPOCHS}]")

    for (x_l, y_l), (x_u, _) in pbar:
        x_l, y_l = x_l.to(DEVICE), y_l.to(DEVICE)
        x_u = x_u.to(DEVICE)

        # === forward ===
        out_student_l, out_glb_l, out_loc_l,_ = student(x_l)
        # è®¡ç®—æ¯ä¸ªåˆ†æ”¯çš„æŸå¤±
        loss_glb = bce(out_glb_l, y_l)  # å…¨å±€åˆ†æ”¯æŸå¤±
        loss_loc = bce(out_loc_l, y_l)  # å±€éƒ¨åˆ†æ”¯æŸå¤±

        # æ€»çš„æœ‰ç›‘ç£æŸå¤±
        loss_sup = loss_glb + loss_loc
        #loss_sup = bce(out_student_l, y_l)

        if epoch >= WARMUP_EPOCHS:
            with torch.no_grad():
                #print("x_u shape:", x_u.shape)
                pred_A = torch.sigmoid(teacher_A(x_u))
                pred_B = torch.sigmoid(teacher_B(x_u))
                # è®¡ç®—æ•™å¸ˆä¸€è‡´æ€§æŸå¤±ï¼ˆL1 æŸå¤±ï¼‰
                teacher_consistency_loss = torch.mean(torch.abs(pred_A - pred_B))  # è®¡ç®—æ•™å¸ˆAå’Œæ•™å¸ˆBçš„L1æŸå¤±

                #uncert_A = compute_uncertainty(teacher_A, x_u)
                #uncert_B = compute_uncertainty(teacher_B, x_u)
                #print("âœ… Before second pred_A - teacher_A.conv1:", teacher_A.conv1[0])

                # dominant, wA, wB, confA, confB = select_dominant_teacher(pred_A, pred_B,
                #     uncertainty_A=uncert_A, uncertainty_B=uncert_B,
                #     epoch=epoch, total_epochs=EPOCHS)
                dominant, wA, wB, confA, confB = select_dominant_teacher(
                    pred_A, pred_B,
                    epoch=epoch,
                    total_epochs=EPOCHS
                )
                #
                # pseudo_label = wA * pred_A + wB * pred_B


            # out_student_u, _, _ = student(x_u)

            # è·å–å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºï¼ˆåŒ…æ‹¬ä¼ªæ ‡ç­¾å’Œæ³¨æ„åŠ›å›¾ï¼‰
            out_student_u, out_glb_u, out_loc_u, attention_map = student(x_u)
            # æ ¹æ® attention_map è®¡ç®—æ¯ä¸ªåƒç´ çš„ä¼ªæ ‡ç­¾
            pseudo_label = attention_map * pred_A + (1 - attention_map) * pred_B
            # print(pseudo_label.min(), pseudo_label.max())
            # print("pseudo_label shape:", pseudo_label.shape)  # ç¡®ä¿å®ƒæ˜¯ [batch_size, channels, height, width]
            # print("out_student_u min/max:", out_student_u.min(), out_student_u.max())
            # print("pseudo_label min/max:", pseudo_label.min(), pseudo_label.max())

            # **é«˜ç½®ä¿¡åº¦ç­›é€‰**ï¼šç”Ÿæˆé«˜ç½®ä¿¡åº¦åŒºåŸŸçš„ mask
            confidence_mask = (pseudo_label > 0.9) | (pseudo_label < 0.1)  # ä¿ç•™æ¥è¿‘0å’Œ1çš„åŒºåŸŸ

            # loss_unsup = bce(out_student_u, pseudo_label)
            # è®¡ç®—æ— ç›‘ç£æŸå¤±ï¼Œå¹¶åªä¿ç•™é«˜ç½®ä¿¡åº¦åŒºåŸŸ
            loss_unsup = bce(out_student_u, pseudo_label)  # åŸå§‹æ— ç›‘ç£æŸå¤±
            loss_unsup = loss_unsup * confidence_mask.float()  # åªä¿ç•™é«˜ç½®ä¿¡åŒºåŸŸ
            loss_unsup = loss_unsup.sum() / (confidence_mask.sum() + 1e-6)  # å½’ä¸€åŒ–

            # **åŠ å…¥è¾¹ç•ŒæŸå¤±**ï¼šè®¡ç®—è¾¹ç•ŒæŸå¤±
            boundary_loss = compute_boundary_loss(out_student_l, y_l)  # å¯¹æœ‰ç›‘ç£éƒ¨åˆ†è®¡ç®—è¾¹ç•ŒæŸå¤±
            hausdorff_loss = hausdorff_distance(out_student_l, y_l)  # è®¡ç®—HausdorffæŸå¤±
            # å°†æ•™å¸ˆä¸€è‡´æ€§æŸå¤±æ·»åŠ åˆ°æ€»æŸå¤±ä¸­
            lambda_teacher_consistency = 0.1  # å¯ä»¥è°ƒæ•´æ­¤è¶…å‚æ•°
            lambda_boundary = 0.5  # è¾¹ç•ŒæŸå¤±çš„æƒé‡
            lambda_hd = 1.0  # HausdorffæŸå¤±çš„æƒé‡

            #loss = loss_sup + loss_unsup + lambda_teacher_consistency * teacher_consistency_loss
            loss = loss_sup + loss_unsup + lambda_teacher_consistency * teacher_consistency_loss + lambda_boundary * boundary_loss + lambda_hd * hausdorff_loss
            #loss = loss_sup + loss_unsup + lambda_teacher_consistency * teacher_consistency_loss + lambda_boundary * boundary_loss
            # loss = loss_sup + loss_unsup
            #loss = loss_sup + loss_unsup
        else:
            loss_unsup = torch.tensor(0.0).to(DEVICE)
            loss = loss_sup

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # === EMA ===
        if epoch >= WARMUP_EPOCHS:
            if dominant == "A":
                smart_ema_update(student.branch_global, teacher_A, uncertainty_map=1 - confA)
            else:
                smart_ema_update(student.branch_local, teacher_B, uncertainty_map=1 - confB)

        #pbar.set_postfix({"SupLoss": loss_sup.item(), "UnsupLoss": loss_unsup.item()})
        # pbar.set_postfix({
        #     "SupLoss": loss_sup.item(),
        #     "UnsupLoss": loss_unsup.item() if epoch >= WARMUP_EPOCHS else 0.0
        # })
        pbar.set_postfix({
            "Loss": loss.item(),
        })
    # ==== Validation ====
    student.eval()
    dice_scores, iou_scores, hd_scores = [], [], []

    with torch.no_grad():
        for x_val, y_val in val_loader:
            x_val, y_val = x_val.to(DEVICE), y_val.to(DEVICE)
            out, _, _,_ = student(x_val)
            pred = (out > 0.5).float()
            dice_scores.append(dice_score(pred, y_val))
            iou_scores.append(iou_score(pred, y_val))
            hd_scores.append(hd95(pred, y_val))

    dice_avg = (sum(dice_scores) / len(dice_scores))
    iou_avg = sum(iou_scores) / len(iou_scores)
    hd_avg = sum(hd_scores) / len(hd_scores)
    #dice_avg,iou_avg,hd_avg = ai(dice_scores, iou_scores, hd_scores)

    print(f"Validation Dice: {dice_avg:.4f}, IoU: {iou_avg:.4f}, HD95: {hd_avg:.2f}")

    # Save best model
    if dice_avg > best_dice:
        best_dice = dice_avg
        torch.save(student.state_dict(), os.path.join(CHECKPOINT_DIR, "student_best8.pth"))
        print(f"âœ… Saved new best model with Dice {best_dice:.4f}")
    # è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨è°ƒæ•´å­¦ä¹ ç‡
    scheduler.step(dice_avg)  # è¿™é‡Œä¼ å…¥çš„æ˜¯å½“å‰çš„ Dice æŒ‡æ ‡